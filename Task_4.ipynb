{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm7PLSIuFqLOD19SvWACk8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NagamallaVinay/Task-4/blob/main/Task_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 4: Classification with Logistic Regression**.\n",
        "\n",
        "Objective: Build a binary classifier using logistic regression.\n",
        "\n",
        "Tools:  Scikit-learn, Pandas, Matplotlib"
      ],
      "metadata": {
        "id": "qyGozQ-gaMBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1: Choose a binary classification dataset.**\n",
        "We will use the provided data.csv file, which contains features for a binary classification task"
      ],
      "metadata": {
        "id": "pBaLhzzxaqGu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ERQ9lzlaJMR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing .CSV file into Notebook**"
      ],
      "metadata": {
        "id": "Q25qptczdHFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "at_jvrpHdCrg",
        "outputId": "446f6353-0005-4857-d69a-e4d39dbe5793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aa8fbe1d-f2d6-47fe-a76b-687d5e3a2ad1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aa8fbe1d-f2d6-47fe-a76b-687d5e3a2ad1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data.csv to data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data.csv')\n",
        "print(\"Dataset loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXyuRsPZd8Sh",
        "outputId": "ba6c568f-40eb-4d66-dd4c-893c9d26a567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target (y)\n",
        "# Dropping 'id' as it's an identifier and 'diagnosis' for the target variable.\n",
        "X = df.drop(['id', 'diagnosis'], axis=1)\n",
        "y = df['diagnosis']\n",
        "\n",
        "# Encode the target variable: 'M' (Malignant) as 1, 'B' (Benign) as 0\n",
        "y_encoded = y.map({'M': 1, 'B': 0})\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"Target value counts:\\n{y.value_counts()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDQnwYWmdU-v",
        "outputId": "643fa3a8-e909-4acb-ad0e-40856b5e252b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (569, 33)\n",
            "Features shape: (569, 31)\n",
            "Target shape: (569,)\n",
            "Target value counts:\n",
            "diagnosis\n",
            "B    357\n",
            "M    212\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2: Train/test split and standardize features**.\n",
        "\n",
        "We will split the data into training and testing sets, and then standardize the features to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "A8S0BJkvbG_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training (80%) and testing (20%) sets\n",
        "# stratify=y_encoded ensures that the class distribution is preserved in both sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "print(\"\\nStep 2: Train-test split and feature standardization.\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Features standardized successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBReSMY4bbf3",
        "outputId": "cc23c95f-b6ec-4025-8a1e-f431396d8ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Train-test split and feature standardization.\n",
            "X_train shape: (455, 31)\n",
            "X_test shape: (114, 31)\n",
            "y_train shape: (455,)\n",
            "y_test shape: (114,)\n",
            "Features standardized successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3: Fit a Logistic Regression model.**\n",
        "\n",
        "We will initialize and train a Logistic Regression model using the standardized training data."
      ],
      "metadata": {
        "id": "611J_UFHbgqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Regenerated Step 3 ---\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np # Ensure numpy is imported for potential array handling\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "# Using 'liblinear' solver is generally a good default for binary classification\n",
        "# and handles smaller datasets well. 'random_state' ensures reproducibility.\n",
        "# For larger datasets, 'lbfgs' or 'saga' might be more efficient.\n",
        "# 'max_iter' can be increased if convergence warnings appear, though unlikely here.\n",
        "try:\n",
        "    model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "    # Train the model on the scaled training data\n",
        "    # X_train_scaled is expected to be a numpy array from StandardScaler\n",
        "    # y_train is expected to be a pandas Series or numpy array of 0s and 1s\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    print(\"\\nStep 3: Logistic Regression model fitted successfully.\")\n",
        "\n",
        "except ValueError as ve:\n",
        "    print(f\"\\nError in Step 3 during model fitting: {ve}\")\n",
        "    print(\"Please check the shapes and types of X_train_scaled and y_train.\")\n",
        "    print(f\"X_train_scaled shape: {X_train_scaled.shape}, dtype: {X_train_scaled.dtype}\")\n",
        "    print(f\"y_train shape: {y_train.shape}, dtype: {y_train.dtype}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred in Step 3: {e}\")\n",
        "    # You might want to print more details about the error if needed\n",
        "    # import traceback\n",
        "    # traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEFOPHAfbycj",
        "outputId": "e674e0bf-5f0e-492d-bdfd-e997eb6a78b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error in Step 3 during model fitting: Input X contains NaN.\n",
            "LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
            "Please check the shapes and types of X_train_scaled and y_train.\n",
            "X_train_scaled shape: (455, 31), dtype: float64\n",
            "y_train shape: (455,), dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4: Evaluate with confusion matrix, precision, recall, ROC-AUC.**\n",
        "\n",
        "We will use the trained model to make predictions on the test set and then evaluate its performance using standard classification metrics."
      ],
      "metadata": {
        "id": "uHlSPQzBb8zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Regenerated Step 4 ---\n",
        "# This step depends on the successful completion of Step 3.\n",
        "\n",
        "if 'model' in locals(): # Check if the model object was successfully created and fitted\n",
        "    print(\"\\nStep 4: Evaluating the model.\")\n",
        "\n",
        "    try:\n",
        "        # Make predictions on the scaled test set\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "        # Predict probabilities for the positive class (Malignant = 1)\n",
        "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "        # --- Data Validation before metrics calculation ---\n",
        "        print(\"Validating data for evaluation...\")\n",
        "\n",
        "        # Validate y_test\n",
        "        if not (y_test.isin([0, 1]).all()):\n",
        "            print(\"Error: y_test contains values other than 0 or 1. Check target encoding.\")\n",
        "            print(f\"Unique values in y_test: {y_test.unique()}\")\n",
        "        else:\n",
        "            print(\"y_test validation passed.\")\n",
        "\n",
        "        # Validate y_pred\n",
        "        if not (np.isin(y_pred, [0, 1]).all()):\n",
        "            print(\"Error: y_pred contains values other than 0 or 1. Check model predictions.\")\n",
        "            print(f\"Unique values in y_pred: {y_pred}\")\n",
        "        else:\n",
        "            print(\"y_pred validation passed.\")\n",
        "\n",
        "        # Validate y_pred_proba\n",
        "        if not (np.issubdtype(y_pred_proba.dtype, np.number)):\n",
        "            print(\"Error: y_pred_proba is not numerical.\")\n",
        "        elif not ((y_pred_proba >= 0) & (y_pred_proba <= 1)).all():\n",
        "            print(\"Error: y_pred_proba contains values outside the [0, 1] range.\")\n",
        "            print(f\"Min probability: {y_pred_proba.min()}, Max probability: {y_pred_proba.max()}\")\n",
        "        else:\n",
        "            print(\"y_pred_proba validation passed.\")\n",
        "\n",
        "        # Ensure shapes are compatible for metrics\n",
        "        if y_test.shape[0] != y_pred.shape[0] or y_test.shape[0] != y_pred_proba.shape[0]:\n",
        "            print(\"Error: Mismatch in the number of samples between y_test, y_pred, and y_pred_proba.\")\n",
        "            print(f\"Shapes: y_test={y_test.shape}, y_pred={y_pred.shape}, y_pred_proba={y_pred_proba.shape}\")\n",
        "        else:\n",
        "            print(\"Sample count validation passed.\")\n",
        "\n",
        "        # Proceed with metrics only if validations pass\n",
        "        print(\"\\nProceeding with metric calculations...\")\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(cm)\n",
        "\n",
        "        # Plot Confusion Matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 12},\n",
        "                    xticklabels=['Benign (0)', 'Malignant (1)'], yticklabels=['Benign (0)', 'Malignant (1)'])\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.show()\n",
        "\n",
        "        # Classification Report (Precision, Recall, F1-Score)\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred, target_names=['Benign (0)', 'Malignant (1)']))\n",
        "\n",
        "        # ROC Curve and AUC\n",
        "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "        # Plot ROC Curve\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='No Skill Classifier')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError occurred during evaluation metrics calculation in Step 4: {e}\")\n",
        "        print(\"This might indicate an issue with the data passed to the metrics functions.\")\n",
        "        # Optional: print detailed traceback for debugging\n",
        "        # import traceback\n",
        "        # traceback.print_exc()\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Step 4 evaluation as the model fitting in Step 3 failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZRA5iscb_-p",
        "outputId": "a3a5403f-4c8a-4129-e21d-64471e470121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4: Evaluating the model.\n",
            "\n",
            "Error occurred during evaluation metrics calculation in Step 4: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
            "This might indicate an issue with the data passed to the metrics functions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5: Tune threshold and explain sigmoid function.**\n",
        "\n",
        "We will demonstrate how changing the classification threshold affects performance and then explain the sigmoid function.\n",
        "\n",
        "The default threshold for logistic regression is 0.5. Let's see how changing it to 0.3 impacts precision and recall."
      ],
      "metadata": {
        "id": "1ikrE3recKs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'model' in locals() and 'y_pred_proba' in locals(): # Check if model and probabilities are available\n",
        "\n",
        "    new_threshold = 0.3\n",
        "\n",
        "    # Predict class labels using the new threshold\n",
        "    # If probability of positive class is >= new_threshold, predict 1 (Malignant), else 0 (Benign).\n",
        "    y_pred_new_threshold = (y_pred_proba >= new_threshold).astype(int)\n",
        "\n",
        "    print(\"\\nStep 5: Demonstrating threshold tuning and explaining sigmoid function.\")\n",
        "    print(f\"\\nClassification Report with Threshold = {new_threshold}:\")\n",
        "    try:\n",
        "        print(classification_report(y_test, y_pred_new_threshold, target_names=['Benign (0)', 'Malignant (1)']))\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating Classification Report with new threshold: {e}\")\n",
        "\n",
        "    # --- 5.2. Explanation of the Sigmoid Function ---\n",
        "    print(\"\\n--- Explanation of the Sigmoid Function ---\")\n",
        "    print(\"The sigmoid function, also known as the logistic function, is crucial in logistic regression.\")\n",
        "    print(\"It squashes any real-valued input into a value between 0 and 1, representing a probability.\")\n",
        "    print(\"\\nFormula:  Ïƒ(z) = 1 / (1 + e^(-z))\")\n",
        "    print(\"\\nWhere 'z' is the linear combination of input features and their weights (z = w0 + w1*x1 + ...).\")\n",
        "    print(\"\\nIn logistic regression, the sigmoid function transforms this linear output into a probability\")\n",
        "    print(\"of belonging to the positive class. This probability is then used with a threshold (e.g., 0.5)\")\n",
        "    print(\"to make the final binary classification.\")\n",
        "    print(\"\\nBy adjusting the threshold, we can tune the model's sensitivity to correctly classify positive instances\")\n",
        "    print(\"(recall) versus the accuracy of its positive predictions (precision).\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping Step 5 as the model fitting or probability prediction failed in previous steps.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kcuyp7mYca_p",
        "outputId": "e044b895-269d-43c2-b6f4-12e88775c2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Skipping Step 5 as the model fitting or probability prediction failed in previous steps.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **sigmoid function**\n",
        "The sigmoid function  is fundamental to logistic regression. It takes any real-valued input and maps it to an output between 0 and 1, which can be interpreted as a probability."
      ],
      "metadata": {
        "id": "3t1xJS_ycwyW"
      }
    }
  ]
}